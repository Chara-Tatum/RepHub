Algorithm prioritized white patients:

An algorithm by Optum was designed to assign a risk value to patients in order to prioritize those who might need extra care. However, black people tended to be marked as “lower risk” than white patients by algorithm, and thus they were not assigned extra care.
One of the variables used by the algorithm was the amount of money spent on health care per year.
On average, money spent on health care by black people was $1800 less per year than the amount spent by white people with similar health problems.

Proportion of patients that were assigned extra care who were black: 17.7%
Estimated proportion of patients that needed extra care who were black: 46.5%

Using health care spending as a variable in this algorithm made it biased because it can be directly tied to systemic racism.
When building ML algorithms, it’s important to examine each individual variable for biases.

Tying it all together:
A higher proportion of black people report fair/poor health condition, a higher proportion of black infants die, a higher proportion of the black population is being hospitalized for COVID-19. So why aren’t black people getting the health care they need?

Possible solutions:
	Better data. We need data that better represents all parties involved in these algorithms. Algorithms such as this one often rely on incomplete data, forcing it to make false assumptions about underrepresented minority peoples and topics.
	Transparency. Companies and government programs that intend to use algorithms to make predictions need to make them public and hold them accountable for racially biased algorithms - intentional or not.
	Understand the data. In our example, the problem was a result of the program developers failing to recognize the role that systemic racism played in their variables. By recognizing and understanding these “hidden biases”, we are less likely to perpetuate disparity in code.

Stats and figures:
